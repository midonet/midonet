# Copyright 2014 Midokura SARL
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Midolman configuration file

[vrn]
router_network_id = 01234567-0123-0123-aaaa-0123456789ab

[zookeeper]
#zookeeper_hosts = 192.168.100.8:2181,192.168.100.9:2181,192.168.100.10:2181
zookeeper_hosts = 127.0.0.1:2181
session_timeout = 30000
midolman_root_key = /midonet/v1
session_gracetime = 30000

[cassandra]
# The minimum recommended cassandra setup is a 3-node cluster with a
# replication factor of 3. Midolman uses Quorum as consistency policy, which
# would translate to 2 in the suggested setup.
#
# Refer to the docs/cassandra-cache.md documentation for more specific details
#servers = 192.168.100.8:9160,192.168.100.9:9160,192.168.100.10:9160
servers = 127.0.0.1:9160
# DO CHANGE THIS, recommended value is 3
replication_factor = 1
cluster = midonet
# How many conns we keep alive at once, on each host. Will start as the given
# value / 3, and grow up to the given max.
max_active_connections=3
# Determines how long we wait for operations to reply, gets passed down to
# the thrift lib dealing with transport
thrift_socket_timeout=2500
# These determine how many timeouts are allowed in a given host within the given
# window (in ms) before the node gets suspended and becomes non eligible for
# future operations.
host_timeout_tracker=true
host_timeout_counter=10
host_timeout_window=500

[bridge]
mac_port_mapping_expire_millis = 15000

[arptable]
arp_retry_interval_seconds = 10
arp_timeout_seconds = 60
arp_stale_seconds = 1800
arp_expiration_seconds = 3600

[midolman]
disconnected_ttl_seconds = 30
control_interface = eth0
cache_type = cassandra
check_flow_expiration_interval = 10000 #millis
# top_level_actor_supervisor = resume
top_level_actor_supervisor = crash
# after requesting an update to the kernel if a flow with idle expiration set
# has less then idle_flow_tolerance_interval to live, we expire it
# idle_flow_tolerance_interval = 10000

# bgpd options

# path to directory containing bgpd binary, default is /usr/sbin
#bgpd_binary = /usr/sbin            # for RHEL
#bgpd_binary = /usr/lib/quagga/     # for ubuntu

# path to directory containing bgpd.conf configuration file for bgpd
#bgpd_config = /etc/quagga  # default value

# number of threads dedicated to packet processing
simulation_threads = 2

# number of datapath output channels
output_channels = 2

# threading model for datapath input channels. There is one channel per port.
# Allowed values are:
#   + one_to_many: use one thread to service all ports
#   + one_to_one: use one thread to service each port
input_channel_threading = one_to_many

# dashboard, experimental
enable_dashboard=false
jetty_xml=/etc/midolman/jetty/etc/jetty.xml

[host]
# This file holds the host UUID across reboots. It is created when
# midolman is first executed in the host, by default it will be stored
# in /etc/midolman/
#properties_file = /etc/midolman/host_uuid.properties
wait_time_between_scans = 5000       # 5 * 1000 millis

[datapath]

# Uncommenting this option will make midolman creates a vxlan tunnel port in
# the 'midonet' datapath at startup, with the specified given udp port. If the
# port value is out of range, the vxlan tunnel port will not be created.
# This feature requires openvswitch-datapath-dkms version 1.10 or above.
#vxlan_udp_port = 4789

# Maximum number of flows a given datapath will be able to contain.
max_flow_count = 100000
# Maximum number of wildcard flows a given datapath will be able to contain.
max_wildcard_flow_count = 100000
# Midolman uses a pool of reusable buffers to send requests to the
# datapath. The options below tune the pool's size and that of its
# buffers. One pool is created for each output channel, the settings
# defined here will apply to each of those pools.
# max_size: maximum number of buffers to hold in the pool. When the
#           pool is empty (all buffers are in use) and has reached
#           its maximum size, temporary buffers will be allocated.
send_buffer_pool_max_size = 2048
# initial_size: initial number of buffers to allocate in the pool
send_buffer_pool_initial_size = 2048
# buf_size_kb: size of each buffer, in kb. Maximum total pool size would thus
#              be: max_size * buf_size_kb. Beware that the buffer size puts a
#              limit on the packet size that Midolman can send. In a network
#              jumbo frames, adjust the size so that one buffer will accomodate
#              a whole frame plus enough room for the flow's actions.
send_buffer_pool_buf_size_kb = 8

# How many datapath messages to process in each batch, increasing througput
# by reducing synchronization costs. Too high a value may hurt latency.
msgs_per_batch = 200


# Midolman limits the amount of packets in flight in the system at any
# given time. This prevents its internal queues from growing infinitely.
# Additionally, midolman ensures that its processing capacity is shared
# fairly among ports connected to the datapath. This, for example,
# would prevent a single VM from setting up new flows at a rate that
# would starve other VMs in the system.
#
# This behaviour is achieved by routing packets that miss the datapath
# flow table and rise to userspace through a Hierarchical Token Bucket.
# This HTB is set up in such a way such that tunnel ports will get 50%
# of the resources, and the remaining 50% is shared fairly among all
# other ports (typically, VMs).
#
# The rate at which the buckets are refilled is automatic and dynamic.
# However the size of the buckets at each stage of the HTB can be tuned
# through the settings below, increasing a bucket size will increase the
# burstiness at which traffic can be queued before new tokens become
# available.

# global_incoming_burts_capacity: size of the root bucket in the HTB.
global_incoming_burst_capacity = 256

# tunnel_incoming_burst_capacity: bucket size for tunnel ports (GRE, VxLAN)
tunnel_incoming_burst_capacity = 128

# vm_incoming_burst_capacity: bucket size for VM ports
vm_incoming_burst_capacity = 32

# vtep_incoming_burst_capacity: bucket size for VTEP (VxLAN) ports.
vtep_incoming_burst_capacity = 128
